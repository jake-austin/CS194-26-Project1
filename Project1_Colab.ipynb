{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1 Colab",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjG4_o5yx6v_"
      },
      "source": [
        "# FilePathing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFGfgKQ0K8J"
      },
      "source": [
        "\"\"\"\n",
        "# This is my code for navigating to my drive to get my images\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/CS194-26/Proj1\"\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TI6GHTHaPw"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Ow3C4_BQxi"
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import skimage as sk\n",
        "from skimage import transform\n",
        "import skimage.io as skio\n",
        "\n",
        "!pip install line_profiler\n",
        "%load_ext line_profiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfa95vj_HqOk"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1kZuhoBrTxe"
      },
      "source": [
        "Just some utils that help with image processing and formatting and io\n",
        "\n",
        "naive_get_colors: returns the r, g, and b arrays by naivley cutting the source image (located at path) into thirds\n",
        "\n",
        "im_from_colors: stacks the r, g, b channels into an image\n",
        "\n",
        "show: just a small version of the existing show method that will increase the figure size automatically, allowing me to look at images in better detail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auubn-bpHrFg"
      },
      "source": [
        "\"\"\"\n",
        "#Replace this code with some way to get the filepaths for all your images\n",
        "\n",
        "image_paths = ['./Images/' + path for path in next(os.walk('./Images'))[2]]\n",
        "big_paths = [path for path in image_paths if 'tif' in path]\n",
        "small_paths = [path for path in image_paths if path not in big_paths]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def naive_get_colors(path):\n",
        "  im = imread(path)\n",
        "  im = im.astype(float)\n",
        "  height = im.shape[0]//3\n",
        "  b = im[:height]\n",
        "  g = im[height: 2*height]\n",
        "  r = im[2*height: 3*height]\n",
        "  return r, g, b\n",
        "\n",
        "def im_from_colors(r, g, b):\n",
        "  return np.dstack([r, g, b])\n",
        "\n",
        "def show(im, figsize = 50, cmap=None):\n",
        "  # Uses matplotlib in the backend to show images\n",
        "  figure(figsize=(figsize,figsize))\n",
        "  imshow(im, cmap=cmap)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_XUXTMPtBPr"
      },
      "source": [
        "Basic Image processing shit:\n",
        "\n",
        "\n",
        "get_offset_maps: given 2 channels (one base and the other offset) return the overlaping parts of the two channels\n",
        "\n",
        "overlaps_filter: given 3 images and all their offsets (x, y) from being all centered, this function returns the overlapping parts of the r, g, and b channels, so the resulting r, g, b overlaps can be stacked and shown as an image. This works whether the order of r, g, b is shifted or not... it will just return them in the same order that they were passed in\n",
        "\n",
        "percent_crops: crops all the edges for r, g, and b by \"percent\" percent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnCpSHOcsiW7"
      },
      "source": [
        "def get_offset_maps(base, offset_map, offset):\n",
        "  \"\"\"\n",
        "  This is a helper function for exhaustive_search, but can be used more generally\n",
        "  to return the part of each map that overlaps, given an offset\n",
        "\n",
        "        base\n",
        "     _______________\n",
        "    |               |\n",
        "    |        _______|_______\n",
        "    |       |       |       |\n",
        "    |_______|_______|       |\n",
        "            |               |\n",
        "            |_______________|\n",
        "\n",
        "            offset map (all positive offsets for x, y in this case)\n",
        "  \"\"\"\n",
        "  x, y = offset\n",
        "\n",
        "  #Note: need to replace 0 in upper bound with None otherwise slicing returns []\n",
        "  #lower bound: if positive, we want positive, if negative we want zero\n",
        "  #upper bound: if positive, we want zero, if negative, we want negative\n",
        "  base = base[max(0, y):min(0, y) if min(0, y) else None,\n",
        "              max(0, x):min(0, x) if min(0, x) else None]\n",
        "\n",
        "  #lower bound: if positive, we want zero, if negative, we want positive\n",
        "  #upper bound: if positive, we want negative, if negative, we want zero\n",
        "  offset_map = offset_map[max(0, -y):min(0, -y) if min(0, -y) else None,\n",
        "                          max(0, -x):min(0, -x) if min(0, -x) else None]\n",
        "\n",
        "  return base, offset_map\n",
        "\n",
        "\n",
        "def overlaps_filter(r, g, b, r_offset, g_offset, b_offset):\n",
        "  offsets = (r_offset, g_offset, b_offset)\n",
        "  height, width = r.shape\n",
        "  \n",
        "  tops = [offset[1] for offset in offsets]\n",
        "  lefts = [offset[0] for offset in offsets]\n",
        "  bottoms = [offset[1] + height for offset in offsets]\n",
        "  rights = [offset[0] + width for offset in offsets]\n",
        "\n",
        "  top = max(tops)\n",
        "  left = max(lefts)\n",
        "  bottom = min(bottoms)\n",
        "  right = min(rights)\n",
        "\n",
        "  #here, instead of doing what we did with the offset maps, we just make all the\n",
        "  # values positive so that we don't have to deal with the case of zero on the upper limit\n",
        "  r = r[top - r_offset[1] : bottom - r_offset[1], left - r_offset[0] : right - r_offset[0] ]\n",
        "  g = g[top - g_offset[1] : bottom - g_offset[1], left - g_offset[0] : right - g_offset[0] ]\n",
        "  b = b[top - b_offset[1] : bottom - b_offset[1], left - b_offset[0] : right - b_offset[0] ]\n",
        "\n",
        "  return r, g, b\n",
        "\n",
        "\n",
        "def percent_crop(r, g, b, percent = .1):\n",
        "  #crops by this percentage\n",
        "  if r.shape != g.shape or g.shape != b.shape:\n",
        "    raise Exception('edge_map_score: Color channels have unequal dimensions')\n",
        "\n",
        "  i1 = int(r.shape[0] * percent)\n",
        "  i2 = int(r.shape[1] * percent)\n",
        "\n",
        "  r = r[i1:-i1, i2:-i2]\n",
        "  g = g[i1:-i1, i2:-i2]\n",
        "  b = b[i1:-i1, i2:-i2]\n",
        "\n",
        "  return r, g, b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4bbmObcIssr"
      },
      "source": [
        "# Main Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TglJfhikrRt8"
      },
      "source": [
        "This is where all the meat is. Here we have all the allignment methods that actually do non-trivial stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmMzoi-JsPW8"
      },
      "source": [
        "Allignment Scores:\n",
        "\n",
        "**These need to be normalized for the number of pixels in the input channels**\n",
        "\n",
        "Our objective should be to **maximize** these scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3osJRDdwsRqp"
      },
      "source": [
        "def l2_score(color1, color2):\n",
        "\n",
        "  if color1.shape != color2.shape:\n",
        "    raise Exception('edge_map_score: Color channels have unequal dimensions')\n",
        "\n",
        "  color1_norm = np.sqrt(np.sum(color1*color1))\n",
        "  color2_norm = np.sqrt(np.sum(color2*color2))\n",
        "\n",
        "  score = np.sum(np.multiply(color1, color2))\n",
        "\n",
        "  return score / (color1_norm * color2_norm)\n",
        "\n",
        "\n",
        "def ssd(color1, color2):\n",
        "  return -sum(sum((color1-color2)**2))/color1.size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ezRLyPhvWMn"
      },
      "source": [
        "exhaustive_search: this will use exhaustive search to find allignment. You will pass in one color channel as the base, and then set color1 and color2 to be the remaining two color channels. We will search for an allignment for both color 1 and color 2 within the ranges x_range_colori and y_range_colori for color i. It also takes in the score function that you want to use. We will use the convention that the score is trying to be maximized. \n",
        "\n",
        "full_exhaustive: takes in a path, your chosen assignments for base, color1, color2, and will plug it all into exhaustive_search, print the resulting image out along with the allignments (if verbose=True)\n",
        "\n",
        "pyramid_search: this will use pyramid search to find allignment. You pass in your base color channel, the two other color channels, a searchrange for x, and y for both channels, a score function, a downsampling parameter that chooses how much you downsample by each time, an error_range that expands the normal window as we recurse back up by error_range amount, and a base case value to tell us at which window size we should stop getting more granular.\n",
        "\n",
        "full_pyramid: another wrapper, same style as full_exhaustive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZFi0PfIwLi"
      },
      "source": [
        "def exhaustive_search(base, color1, color2, x_range_color1 = (-15, 15), y_range_color1 = (-15, 15),\n",
        "                      x_range_color2 = (-15, 15), y_range_color2 = (-15, 15), score_fn = ssd):\n",
        "\n",
        "  #make it so that color1, color2 and corresponding offsets point to the\n",
        "  #underlying color slice and offset lists, which we will use\n",
        "  #only the offset tuples should be modified. The colormaps should stay the same\n",
        "\n",
        "  #do the exhaustive search itself, for color1\n",
        "  best_score = -float('inf')\n",
        "  best_offset = [0, 0]\n",
        "  for x in range(*x_range_color1):\n",
        "    for y in range(*y_range_color1):\n",
        "      temp_base, temp_offset = get_offset_maps(base, color1, (x, y))\n",
        "      score = score_fn(temp_base, temp_offset)\n",
        "      if score > best_score:\n",
        "        best_score = score\n",
        "        best_offset = [x, y]\n",
        "\n",
        "  color1_offset = best_offset\n",
        "\n",
        "  #do the exhaustive search itself, for color2\n",
        "  best_score = -float('inf')\n",
        "  best_offset = [0, 0]\n",
        "  for x in range(*x_range_color2):\n",
        "    for y in range(*y_range_color2):\n",
        "      temp_base, temp_offset = get_offset_maps(base, color2, (x, y))\n",
        "      score = score_fn(temp_base, temp_offset)\n",
        "      if score > best_score:\n",
        "        best_score = score\n",
        "        best_offset = [x, y]\n",
        "\n",
        "  color2_offset = best_offset\n",
        "\n",
        "  return color1_offset, color2_offset\n",
        "\n",
        "\n",
        "def full_exhaustive(path, base_str = 'r', color1_str = 'g', color2_str = 'b', verbose = True):\n",
        "  r, g, b = naive_get_colors(path)\n",
        "  r = r/np.max(r)\n",
        "  g = g/np.max(g)\n",
        "  b = b/np.max(b)\n",
        "\n",
        "  base = eval(base_str)\n",
        "  color1 = eval(color1_str)\n",
        "  color2 = eval(color2_str)\n",
        "\n",
        "  color1_offset, color2_offset = exhaustive_search(*percent_crop(base, color1, color2, .3))\n",
        "\n",
        "  r_offset = (0, 0) if 'r'==base_str else (color1_offset if color1_str == 'r' else color2_offset)\n",
        "  g_offset = (0, 0) if 'g'==base_str else (color1_offset if color1_str == 'g' else color2_offset)\n",
        "  b_offset = (0, 0) if 'b'==base_str else (color1_offset if color1_str == 'b' else color2_offset)\n",
        "\n",
        "  if verbose:\n",
        "    print('Color offsets:', 'Red: ', tuple(r_offset), 'Green: ', tuple(g_offset), 'Blue: ', tuple(b_offset))\n",
        "  r, g, b = overlaps_filter(r, g, b, r_offset, g_offset, b_offset)\n",
        "  show(im_from_colors(r, g, b))\n",
        "\n",
        "\n",
        "def pyramid_search(base, color1, color2, searchrange = (-150, 150), score_fn = ssd, downsampling = 2, error_range = 0, base_case=10):\n",
        "  #error range is the extra pixels we check on either side of the range to account for extra errors and wonkyness in the image\n",
        "  #we will recurse when the range <= base_case\n",
        "\n",
        "  #our base case\n",
        "  if searchrange[1] - searchrange[0] <= base_case:\n",
        "    color1_offset, color2_offset = exhaustive_search(base, color1, color2, searchrange, searchrange,\n",
        "                                                     searchrange, searchrange, score_fn)\n",
        "    return color1_offset, color2_offset\n",
        "  \n",
        "  #recursive case\n",
        "  #get offsets for the downsampled image, modifying the range and the colors accordingly\n",
        "  color1_offset, color2_offset = pyramid_search(base = sk.transform.rescale(base, 1/downsampling), \n",
        "                                                color1 = sk.transform.rescale(color1, 1/downsampling),\n",
        "                                                color2 = sk.transform.rescale(color2, 1/downsampling), \n",
        "                                                searchrange = (searchrange[0]//downsampling, searchrange[1]//downsampling),\n",
        "                                                score_fn = score_fn, downsampling = downsampling, \n",
        "                                                error_range = error_range)\n",
        "\n",
        "  #upscale the offsets\n",
        "  color1_offset = (color1_offset[0] * downsampling, color1_offset[1] * downsampling)\n",
        "  color2_offset = (color2_offset[0] * downsampling, color2_offset[1] * downsampling)\n",
        "\n",
        "  #get our ranges that the new offset could feasibly be in... plus the extra error_range\n",
        "  x_range_color1 = (color1_offset[0] - (downsampling + error_range - 1), color1_offset[0] + downsampling + error_range)\n",
        "  y_range_color1 = (color1_offset[1] - (downsampling + error_range - 1), color1_offset[1] + downsampling + error_range)\n",
        "  x_range_color2 = (color2_offset[0] - (downsampling + error_range - 1), color2_offset[0] + downsampling + error_range)\n",
        "  y_range_color2 = (color2_offset[1] - (downsampling + error_range - 1), color2_offset[1] + downsampling + error_range)\n",
        "  \n",
        "  return exhaustive_search(base, color1, color2, x_range_color1, y_range_color1,\n",
        "                           x_range_color2, y_range_color2, score_fn)\n",
        "\n",
        "\n",
        "def full_pyramid(path, base_str = 'r', color1_str = 'g', color2_str = 'b', verbose = True, searchrange=(-150, 150)):\n",
        "  r, g, b = naive_get_colors(path)\n",
        "  r = r/np.max(r)\n",
        "  g = g/np.max(g)\n",
        "  b = b/np.max(b)\n",
        "\n",
        "  base = eval(base_str)\n",
        "  color1 = eval(color1_str)\n",
        "  color2 = eval(color2_str)\n",
        "\n",
        "  color1_offset, color2_offset = pyramid_search(*percent_crop(base, color1, color2, .1), searchrange = searchrange)\n",
        "\n",
        "  r_offset = (0, 0) if 'r'==base_str else (color1_offset if color1_str == 'r' else color2_offset)\n",
        "  g_offset = (0, 0) if 'g'==base_str else (color1_offset if color1_str == 'g' else color2_offset)\n",
        "  b_offset = (0, 0) if 'b'==base_str else (color1_offset if color1_str == 'b' else color2_offset)\n",
        "\n",
        "  if verbose:\n",
        "    print('Color offsets:', 'Red: ', tuple(r_offset), 'Green: ', tuple(g_offset), 'Blue: ', tuple(b_offset))\n",
        "  r, g, b = overlaps_filter(r, g, b, r_offset, g_offset, b_offset)\n",
        "  show(im_from_colors(r, g, b), figsize=20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEHJ6S55ud1L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0qFGE_rueMV"
      },
      "source": [
        "# Bells and Whistles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdio05HEuieY"
      },
      "source": [
        "Image Preprocessing\n",
        "\n",
        "edge_maps: this takes in r, g, b and comes up with a pretty comprehensive edge map by taking the absolute value of color gradients in the x, y, and xy directions (as opposed to the basic 'gradient' kernel with an 8 in the middle and -1 elsewhere). It also clips the gradients to a minimum and maximum number of standard deviations from the mean gradient. This function gives better r, g, b images to come up with offsets with, since color changes in the same places for all channels, even if the brightness of all areas isnt the same\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cblf8Y044LZx"
      },
      "source": [
        "def edge_maps(r, g, b, clip_stds = (0, float('inf'))):\n",
        "  #clip clips the edges at a certain std value and then rescales accordingly\n",
        "\n",
        "  #r = cv2.GaussianBlur(r, (5,5), 0)\n",
        "  #g = cv2.GaussianBlur(g, (5,5), 0)\n",
        "  #b = cv2.GaussianBlur(b, (5,5), 0)\n",
        "\n",
        "  x_edges = [cv2.Sobel(src=img, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) for img in (r, g, b)]\n",
        "  y_edges = [cv2.Sobel(src=img, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5) for img in (r, g, b)]\n",
        "  xy_edges = [cv2.Sobel(src=img, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) for img in (r, g, b)]\n",
        "\n",
        "  r = abs(x_edges[0]) + abs(y_edges[0]) + abs(xy_edges[0])\n",
        "  g = abs(x_edges[1]) + abs(y_edges[1]) + abs(xy_edges[1])\n",
        "  b = abs(x_edges[2]) + abs(y_edges[2]) + abs(xy_edges[2])\n",
        "\n",
        "  r_std = np.std(r)\n",
        "  r = np.clip(r, clip_stds[0] * r_std, clip_stds[1] * r_std)\n",
        "  r = r/np.max(r)\n",
        "\n",
        "  g_std = np.std(g)\n",
        "  g = np.clip(g, clip_stds[0] * g_std, clip_stds[1] * g_std)\n",
        "  g = g/np.max(g)\n",
        "\n",
        "  b_std = np.std(b)\n",
        "  b = np.clip(b, clip_stds[0] * b_std, clip_stds[1] * b_std)\n",
        "  b = b/np.max(b)\n",
        "\n",
        "  return r, g, b\n",
        "\n",
        "def edge_pyramid(path, base_str = 'r', color1_str = 'g', color2_str = 'b', verbose = True, searchrange=(-150, 150)):\n",
        "  r, g, b = naive_get_colors(path)\n",
        "  r = r/np.max(r)\n",
        "  g = g/np.max(g)\n",
        "  b = b/np.max(b)\n",
        "\n",
        "  base = eval(base_str)\n",
        "  color1 = eval(color1_str)\n",
        "  color2 = eval(color2_str)\n",
        "\n",
        "  color1_offset, color2_offset = pyramid_search(*edge_maps(*percent_crop(base, color1, color2, .1), (0, 4)), searchrange = searchrange)\n",
        "\n",
        "  r_offset = (0, 0) if 'r'==base_str else (color1_offset if color1_str == 'r' else color2_offset)\n",
        "  g_offset = (0, 0) if 'g'==base_str else (color1_offset if color1_str == 'g' else color2_offset)\n",
        "  b_offset = (0, 0) if 'b'==base_str else (color1_offset if color1_str == 'b' else color2_offset)\n",
        "\n",
        "  if verbose:\n",
        "    print('Color offsets:', 'Red: ', tuple(r_offset), 'Green: ', tuple(g_offset), 'Blue: ', tuple(b_offset))\n",
        "  r, g, b = overlaps_filter(r, g, b, r_offset, g_offset, b_offset)\n",
        "  show(im_from_colors(r, g, b), figsize=20)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX7KMXjbrMbZ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvgh7by8rNCG"
      },
      "source": [
        "# It is assumed that at this point, you have a \n",
        "#   list 'image_paths' of all image paths\n",
        "\n",
        "len(image_paths)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86JJKv0VzEIB"
      },
      "source": [
        "i=0\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLcQJtm86GW0"
      },
      "source": [
        "i=1\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7tJQRw38dxH"
      },
      "source": [
        "i=2\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BkvF-yz8l_R"
      },
      "source": [
        "i=3\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'r', 'g', 'b', searchrange=(-15, 15))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniHTaGS8nNC"
      },
      "source": [
        "i=4\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_FzTqDL8oVB"
      },
      "source": [
        "i=5\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g', searchrange = (-15, 15))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOsrVtta8rTS"
      },
      "source": [
        "i=6\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g', searchrange=(-15, 15))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6I1pfrT9QK5"
      },
      "source": [
        "i=7\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rif5YwH49ZwC"
      },
      "source": [
        "i=8\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'r', 'g', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kWiqYGy9iMy"
      },
      "source": [
        "i=9\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSgSxNQE9jRL"
      },
      "source": [
        "i=10\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'r', 'g', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq0AtcAB9kPR"
      },
      "source": [
        "i=11\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulP6K5ZF9lGB"
      },
      "source": [
        "i=12\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66fB0aer9l33"
      },
      "source": [
        "i=13\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffb6gOls9miZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxoLnFiDSNHi"
      },
      "source": [
        "Other Images in the Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRWj0rjGSPFz"
      },
      "source": [
        "full_pyramid('./Images/other1.tif', 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evh4rTp5S8yA"
      },
      "source": [
        "full_pyramid('./Images/other2.tif', 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YHr6y2nS-EZ"
      },
      "source": [
        "full_pyramid('./Images/other3.tif', 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN7qefTFS-T-"
      },
      "source": [
        "full_pyramid('./Images/other4.tif', 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAo8dh_cS-cb"
      },
      "source": [
        "full_pyramid('./Images/other5.tif', 'g', 'r', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoCR30C6F4qs"
      },
      "source": [
        "# Bells and Whistles Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgka3iu2GHa6"
      },
      "source": [
        "Here we can see the regular algorithm failing on the emir. The first photo has yellow smearing on the sides of the man. The second is even worse. These are bad selections of base color since the blue is so saturated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHfg57fF6XG"
      },
      "source": [
        "i=1\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'r', 'g', 'b')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpDdvCCSGFQi"
      },
      "source": [
        "i=1\n",
        "print(image_paths[i])\n",
        "full_pyramid(image_paths[i], 'b', 'r', 'g')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxEDHkVxG3Mz"
      },
      "source": [
        "This image uses edge maps to better allign the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKaylxfVGY_a"
      },
      "source": [
        "i=1\n",
        "print(image_paths[i])\n",
        "edge_pyramid(image_paths[i], 'b', 'r', 'g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JTFDe2YHGAH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}